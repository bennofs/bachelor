\chapter{Anforderungen}
\label{chap:requirements}

\section{funktionale Anforderungen}
Im Fokus dieser Arbeit steht die Entwicklung eines Systems zur Filterung des RDF-Exports von Wikidata.
Damit sollen Wissenschaftler mit wenig Aufwand Dumps nach speziellen Kriterien erstellen können.
Die wesentlichen Merkmale des Systems sind:

\begin{description}
  \item[Format] Der Dump soll als RDF im N-Triples Format erstellt werden. Damit die gefilterten Dumps möglichst kompatibel mit dem vollständigen Wikidata RDF Dump sind, sollte das Schema dem offiziellen RDF-Dump-Format entsprechen. Erweiterungen des Schemas müssen klar gekennzeichnet sein.
  \item[Filterung] Nutzer sollen über eine einfache Oberfläche wählen können, welche Entitäten in welchem Detailgrad im erstellten Dump enthalten sind. 
  \item[Archivierung] Damit die Dumps in wissenschaftlichen Veröffentlichungen zitiert werden können, muss die Verfügbarkeit auch in ferner Zukunft garantiert werden. Deshalb ist eine Methode zur Langzeitarchivierung generierter Dumps notwendig. 
  \item[Suche] es soll eine durchsuchbare Übersicht über alle erstellen Dumps.
    So können Ideen anderer Nutzer als Vorlage dienen und nicht jeder Nutzer muss sich eigene Filterregeln ausdenken,
  \item[Statistiken] Um schnell zu entscheiden, ob ein Dump für einen bestimmten Anwendungsfall geeignet ist, sollten Statistiken über den Inhalt (z.B. die Anzahl der Entitäten) des Dumps angezeigt werden. Zusätzlich sollten auch schon während der Generierung Statistiken zum Fortschritt des Dumps bereitgestellt werden.
  \item[Nachvollziehbarkeit] Gerade für den wissenschaftlichen Einsatz ist es erforderlich, dass die Herkunft der Daten und Generierung nachvollziehbar ist. Es sollte demnach leicht sichtbar sein, wie der Dump entstanden ist und eine Reproduktion des Dumps mit diesen Daten möglich sein. 
  \item[Aktualität der Daten] Die Dumps sollten aus möglichst aktuellen Daten erstellt werden.
    Es ist natürlich nicht notwendig, dass die Dumps immer komplett aktuell sind, aber die Daten zur Generierung der Dumps sollten regelmäßig aktualisiert werden.
\end{description}

\section{nicht-funktionale Anforderungen}
Neben den Anforderungen an die Funktion des Systems existieren auch eine Reihe von weiteren Anforderungen:

\begin{description}
  \item[Hardwareanforderungen] Der Ressourcenverbrauch des Systems sollte in einem akzeptablen Rahmen liegen.
  Als Anhaltspunkte für die Beurteilung dienen hier vergleichbare Systeme, wie bspw. der Wikidata Query Service, welcher ebenfalls Services basierend auf den RDF-Daten von Wikidata anbietet und gleichzeitig deutlich populärer ist. Demnach sollte das System entsprechend seiner Relevanz geringere Hardwareanforderungen als der Wikidata Query Service haben.
\item[Freie Lizenz] Die Wikimedia Foundation legt großen Wert darauf, möglichst viele der verwendeten Tools unter einer freien Lizenz bereitzustellen\cite{wikimedia-guiding-principles}.
  Wenn das System in der Wikimedia Cloud betrieben werden soll, dann ist die Veröffentlichung des Quellcodes unter einer freien Lizenz sogar Pflicht\cite{wikimedia-cloud-tos}.
\item[Bearbeitungszeit] Das System sollte maximal einen Tag zur Generierung eines Dumps benötigen, besser unter 12 Stunden. Während längerer Prozesse sollte keine ständig Verfügbarkeit des Nutzers erwartet werden. 
\item[Erweiterbarkeit] Da die funktionalen Anforderungen an das System sehr allgemein sind, muss auf Erweiterbarkeit geachtet werden sodass neue Anforderungen einfach umgesetzt werden können. Es sind beispielsweise viele verschiedene Filtermöglichkeiten und Statistiken denkbar, die nicht alle in der ersten Version umgesetzt werden können. Es ist daher sinnvoll vor allem in diesem Bereich auf Erweiterbarkeit zu achten.
\item[Skalierbarkeit] Wikidata wächst beständig, aktuell existieren etwas weniger als 59 Millionen Items.\TODO{cite}
  Insgesamt gibt es über 700 Millionen Statements und diese Zahl ist allein im letzten Jahr um 200 Millionen gewachsen. Das System muss also mit dieser Menge an Daten umgehen können und dies auch für die nächsten Jahre noch leisten können.
\end{description}

\section{Verwandte Arbeiten}
Zu den verwandten Projekten gehört sicherlich der Wikidata Query Service\cite{wd-sparql}, welcher bereits einen gezielten Zugriff auf die Daten gewährt.
Die Daten des Query Services entsprechen bis auf wenige Minuten Verzögerung der neusten Wikidata Version.
Da der Query Service allerdings beliebige SPARQL-Abfragen erlaubt, ist ein Timeout von einer Minute notwendig um zu großen Ressourcenverbrauch abzufangen, denn bei SPARQL-Abfragen kann die Termination nicht sichergestellt werden.
Für die Erzeugung von großen Dumps ist der Query Service damit ungeeignet.

Zwei weitere existierende APIs, der Linked Data Export von Wikidata und die Wikidata API, skalieren ebenso nicht für größere Abfragen.
Mit dem Linked Data Export können die RDF und JSON Daten einzelner Items über HTTP abgerufen werden.
Die API erlaubt das Abrufen der JSON Daten von bis zu 50 Items in einer Anfrage.
Beide Varianten sind daher nur für den Abruf einer kleinen Menge an Items geeignet.

Ein Mittelweg zwischen Abfragekomplexität für den Server und Anzahl an Abfragen an der Server beschreibt die Linked Data Fragments\cite{ldf} Spezifikation.
Allerdings führt dies zu hohen Abfragekosten auf dem Client, da die Joins auf dem Client ausgewertet werden.
Die Implementierung für Wikidata basiert außerdem auf der selben RDF-Datenbank wie auch der Wikidata Query Service.
Im Kontext von Wikidata haben Linked Data Fragments daher keinen Vorteil gegenüber dem Query Service.

Eine Variante zur Reduzierung der Größe von RDF-Dumps ist HDT\cite{hdt}, einem Binärformat für RDF-Daten.
Es verwendet eine optimierte, komprimierte Kodierung zur Speicherung der Dumps.
Auf diesem Format lassen sich sogar einfache SPARQL-Abfragen effizient ausführen\cite{hdt-query}.
Leider erfordert die Erstellung von HDT-Dumps viel Arbeitsspeicher und Rechenzeit, besonders bei einer so großen Datenmenge wie Wikidata.
Durch die komprimierte Kodierung ist eine inkrementelle Aktualisierung von HDT-Dumps nicht möglich, sodass der komplette Dump regelmäßig neu erstellt werden müsste.

DBpedia\cite{dbpedia} teilt die seine Dumps in mehrere, kleinere Dumps auf.
Auch einige aus Wikidata abgeleitete Dumps sind verfügbar\footnote{\url{https://databus.dbpedia.org/dbpedia/wikidata/}}.
Durch das fest definierte Schema ist eine solche Trennung der Datensätze bei DBpedia möglich.
Für Wikidata ist dieser Ansatz deutlich schwieriger, da das Datenschema so flexibel ist.
Es gibt in Wikidata damit einzelnen Aspekt, nach dem diese Trennung erfolgen könnte.

Ein mächtiges Tool zum Erstellen von Listen von Wikidata Items ist PetScan\footnote{\url{https://petscan.wmflabs.org/}}.
Dieses Tool bietet ein User-Interface mit einer Vielzahl an Optionen, um Listen von Wikipedia bzw. Wikidata-Seiten zu erstellen.
Wikidata ist nicht der Hauptanwendungszweck dieses Tools, dennoch ist es eine interessante Quelle von Filtern.