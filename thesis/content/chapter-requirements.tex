\chapter{Anforderungen}
\label{chap:requirements}
In diesem Kapitel werden die Anforderungen an das System zum Erstellen von gefilterten RDF-Dumps definiert.
Dazu werden zuerst die funktionalen, dann die nicht-funktionalen Anforderungen betrachtet.
Um das System von bereits existierenden Systemen abzugrenzen werden danach verwandte Systeme verglichen.

\section{Funktionale Anforderungen}
Im Fokus dieser Arbeit steht die Entwicklung eines Systems zur Filterung des RDF-Exports von Wikidata.
Damit sollen Anwender mit wenig Aufwand Dumps nach speziellen Kriterien erstellen können.
Die wesentlichen Merkmale des Systems sind:

\begin{description}
  \item[Korrektheit] Der Dump soll als RDF im N-Triples Format erstellt werden. Damit die gefilterten Dumps möglichst kompatibel mit dem vollständigen Wikidata RDF Dump sind, sollte das Schema dem offiziellen RDF-Dump-Format entsprechen. Erweiterungen des Schemas müssen klar gekennzeichnet sein.
  \item[Filterung] Über eine einfache Oberfläche soll eine Filterung nach mindestens drei Aspekten möglich sein: nach Entität (welche Entitäten sollen exportiert werden), nach Property (Statements für welche Properties sollen exportiert werden) und nach Sprache (in welchen Sprachen sollen Zeichenketten exportiert werden). Für Statements soll auswählbar sein, welche Teile exportiert werden (Qualifier, Referenzen, nur truthy Tripel, vollständiges Statement).
  \item[Vollständigkeit] Alle Daten, welche den gewählten Filtern entsprechen, sollten im Dump vorhanden sein. 
  \item[Parallelverarbeitung] Mehrere Anfragen sollen parallel verarbeiten werden. Das bedeutet, dass die Zeit zum Erstellen eines Dumps nicht unbegrenzt mit der Anzahl der Nutzer wächst.
  \item[Archivierung] Damit die Dumps in wissenschaftlichen Veröffentlichungen zitiert werden können, muss die Verfügbarkeit auch in absehbarer Zukunft garantiert werden. Deshalb ist eine Methode zur Langzeitarchivierung generierter Dumps notwendig. 
  \item[Suche] Es soll eine durchsuchbare Übersicht über alle erstellten Dumps geben.
    So können Ideen anderer Nutzer als Vorlage dienen. Das spart Zeit, da keine erneute Spezifikation der einzelnen Filter notwendig ist. Außerdem wird damit die Nutzung vorhandener Dumps gefördert, was die Last auf das System verringert und Vergleichbarkeit in Studien verbessert.
  \item[Statistiken] Um schnell zu entscheiden, ob ein Dump für einen bestimmten Anwendungsfall geeignet ist, sollten Metadaten über den Inhalt (z.B. die Anzahl der Entitäten) des Dumps angezeigt werden. 
  \item[Fortschritt] Während ein Dump generiert wird, soll der Fortschritt des Generierungsprozesses angezeigt werden.
  \item[Nachvollziehbarkeit] Gerade für den wissenschaftlichen Einsatz ist es erforderlich, dass die Herkunft der Daten und Generierung nachvollziehbar ist.
    Es sollte demnach leicht sichtbar sein, wie der Dump entstanden ist und eine Reproduktion des Dumps mit diesen Daten möglich sein. 
  \item[Aktualität der Daten] Die Dumps sollten aus möglichst aktuellen Daten erstellt werden.
    Es ist nicht notwendig, dass die Dumps immer komplett aktuell sind, aber die Daten zur Generierung der Dumps sollten regelmäßig aktualisiert werden.
\end{description}

Als Grundlage zur Ermittlung der Minimalanforderungen für die Filterkriterien werden existierende Arbeiten betrachtet, welche Exporte von Wissengraphen wie Wikidata gefiltert haben.
Ein paar Beispiele dafür sind:
\begin{enumerate}[label=\arabic*)]
  \item nur "`truthy"' Statements mit Entities als Objekt, Labels/Descriptions nur in Englisch und Spanisch\cite{usage-grafa}
  \item nur "`truthy"' simple Statements, wobei sowohl Subjekt als auch Objekt ein Wikidata-Item sind\cite{usage-wembedder}
  \item alle Items mit einem Statement für die Property \verb|P727| (Europeana ID, ID in der europäischen virtuellen Bibliothek „Europeana“ für Kulturobjekte)\cite{usage-europeana}
  \item alle Items mit einem Statement für mindestens eine von 50 festgelegten Properties\cite{usage-narratives} 
  \item nur der englische Teil\cite{usage-web-tables} 
  \item nur Statements für Properties, wobei die Properties Instanzen einer bestimmten Klasse sein müssen, ohne deprecrated, no-value und unknown-value Statements\cite{usage-implicational-knowledge} (allerdings wurde der Dump hier nicht als RDF, sondern in der JSON-Form verarbeitet)
  \item nur Entities von gestorbenen Personen, die an mindestens drei Statements für mindestens 5000 mal vorkommende Properties beteiligt sind\cite{usage-learning-structured-embeddings}
  \item kleinere Datensätze, erzeugt durch zufälliges Auswählen von Statements\cite{usage-sparql-benchmark}
  \item alle Items, die Instanzen (\verb|P31|) der Klassen \verb|Q5| (Mensch) sind und mindestens 6 Fakten (Statements) haben\cite{usage-one-sentence} oder nur die einfachen Statements dieser Personen für die Properties \verb|P27| (Land der Staatsangehörigkeit) und \verb|P106| (Tätigkeit)\cite{usage-person-networks}
  \item zufällige Auswahl von Entities (Sampling), mit englischer Beschreibung und mindestens 5 Statements\cite{usage-generate-entity-type-desc}, optional auch mit Downsampling von Instanzen bestimmter Klassen (um die Verteilung der Klassen im Ergebnis auszugleichen)\cite{usage-synthesize-entity-desc}
  \item alle Statements, welche Referenzen haben, und die Sitelinks der Subjekte dieser Statements\cite{wd-wk-common-references}
\end{enumerate}
Mit den drei beschriebenen Aspekten (Filterung nach Entity, Property und Sprache) und den verschiedenen Optionen für den Export von Statements lassen sich die meisten dieser Anwendungsfälle abdecken.
Außerdem fällt auf, dass die Auswahl von Entities nur auf lokalen Eigenschaften basiert, die anhand des Entity-Dokuments entschieden werden können. 
Kompliziertere Kriterien beschränken sich auf die Auswahl der zu exportierenden Statements.

In den genannten initialen Anforderungen ist eine Filterung nach statistischen Merkmalen, wie die Anzahl der Statements eines Entity, und zufällige Auswahl von Objekten (Sampling) noch nicht enthalten.
Für einen Prototypen sind diese Funktionen nicht notwendig, denn wie die Liste der Anwendungsfälle zeigt, gibt es auch viele interessante Kriterien die diese Funktion nicht benötigen.
Da es jedoch auch mehrere Anwendungsfälle für diese Art der Filterung gibt, sollte das Design um diese Funktionen erweiterbar sein.

\section{Nicht-funktionale Anforderungen}
Die nicht-funktionalen Anforderungen setzen sich zusammen aus Standardanforderungen an Softwareprojekte, wie gute Code-Qualität und Dokumentation, und weitere Anforderungen, die sich aus der konkreten Aufgabe und dem Anwendungsfall als Tool im Kontext eines Wikimedia-Projekts ergeben:
\begin{description}
\item[Hardwareanforderungen] Der Ressourcenverbrauch des Systems sollte in einem akzeptablen Rahmen liegen.
    Als Anhaltspunkte für die Beurteilung dienen hier vergleichbare Systeme, wie der Wikidata Query Service, welcher ebenfalls Dienste basierend auf den RDF-Daten von Wikidata anbietet und gleichzeitig deutlich populärer ist (44 Anfragen pro Sekunde im Jahr 2018 \cite{wd-sparql}). Die Datenbank des Wikidata Query Service war 2018 über 400GB groß. Der Dienst benutzt sechs Server mit folgender Hardware: dual Intel(R) Xeon(R) CPU E5-2620 v4 8 core; 128G RAM; Dual RAID 800G SSD als Speicher. Diese laufen allerdings nicht unter Volllast.
    Das System sollte entsprechend seiner Relevanz geringere Hardwareanforderungen als der Wikidata Query Service haben. Für die initiale Version des Systems sollten 200GiB Festplattenspeicher und 8 GiB Arbeitsspeicher ausreichen. Diese Anforderungen erlauben das Deployment auf der Wikimedia Toolforge\footnote{Die Toolforge erlaubt nach einer einfachen Registration die Nutzung von verschiedenen Cloud-Services der Wikimedia Foundation: \url{https://tools.wmflabs.org/}}. Damit ist das Deployment für die initiale Version einfach.
\item[Skalierbarkeit] Wikidata wächst beständig; Stand September 2019 existieren etwas mehr als 60 Millionen Items\footnote{\url{https://web.archive.org/web/20190930152246/https://www.wikidata.org/wiki/Wikidata:Statistics}}.
\item[Freie Lizenz] Die Wikimedia Foundation legt großen Wert darauf, möglichst viele der verwendeten Tools unter einer freien Lizenz bereitzustellen\cite{wikimedia-guiding-principles}.
  Wenn das System in der Wikimedia Cloud betrieben werden soll, dann ist die Veröffentlichung des Quellcodes unter einer freien Lizenz sogar Pflicht\cite{wikimedia-cloud-tos}.
\item[Bearbeitungszeit] Das System sollte nicht länger als einen Tag zur Generierung eines Dumps benötigen, besser unter 12 Stunden. Falls die Generierung länger als 10 Minuten dauert, muss die Generierung auf einem Server stattfinden, sodass der Rechner des Nutzers währenddessen ausgeschaltet werden kann.
\item[Erweiterbarkeit] Da die funktionalen Anforderungen an das System sehr allgemein sind, muss auf Erweiterbarkeit geachtet werden, sodass neue Anforderungen einfach umgesetzt werden können. Es sind beispielsweise viele verschiedene Filtermöglichkeiten und Statistiken denkbar, die nicht alle in der ersten Version umgesetzt werden können. Es ist daher sinnvoll vor allem in diesem Bereich auf Erweiterbarkeit zu achten.
  Insgesamt gibt es über 700 Millionen Statements\footnote{\url{https://grafana.wikimedia.org/d/000000175/wikidata-datamodel-statements?refresh=30m&orgId=1}} und diese Zahl ist allein im letzten Jahr um 200 Millionen gewachsen. Das System muss mit dieser Menge an Daten umgehen können und dies auch in absehbarer Zukunft noch leisten können.
\item[Code-Qualität] Der Quellcode sollte die gängigen Anforderungen an Wartbarkeit und Lesbarkeit erfüllen. Es muss ausreichend Dokumentation geben, dass auch andere Entwickler an dem Projekt weiterarbeiten können.
\item[Dokumentation] Neben der Dokumentation des Quellcodes (Punkt Code-Qualität) sollte es auch Dokumentation zur Erklärung der Funktionsweise des Systems geben. Nutzer sollten anhand der Dokumentation die Optionen des Systems verstehen und nachvollziehen können, wie die Dumps erzeugt werden.
\item[Technologien] Spezifische Anforderungen an Technologien werden nicht gestellt.
  Allerdings bietet die Wikimedia-Toolforge als Dienste bereits die Datenbank MySQL und Redis an, sodass die Nutzung dieser Technologien mit weniger Aufwand verbunden ist. Als Programmiersprache sind bekanntere Programmiersprachen vorteilhaft, damit die Wartbarkeit in Zukunft durch andere Personen gesichert ist. Bei Verwendung einer wenig verbeiteten Programmiersprache ist die Warscheinlichkeit höher, dass erst eine Einarbeitungszeit für diese spezielle Programmiersprache erforderlich ist.
\end{description}

\section{Verwandte Arbeiten}
Als verwandte Arbeiten werden an dieser Stelle zuerst die verschiedenen, bereits existierenden Schnittstellen zum Zugriff auf Wikidata diskutiert.
Anschließend werden mit DBpedia und HDT zwei Arbeiten betrachtet, welche das Problem der großen Dumps auf andere Art lösen.

Die Wikidata API und der Linked Data Export von Wikidata sind zwei einfache Schnittstellen für den Datenzugriff.
Mit dem Linked Data Export können die RDF und JSON Daten einzelner Items über HTTP abgerufen werden.
Die API erlaubt das Abrufen der JSON Daten von bis zu 50 Items in einer Anfrage.
Bei beiden Schnittstellen muss vorher bekannt sein, auf welche Items zugegriffen werden soll.
Komplexere Kriterien zur Auswahl von Items sind mit dieser Schnittstelle daher nicht möglich.
Die Limitation von Anfragen auf wenige Items beschränkt außerdem die maximale Anzahl an Items die exportiert werden können, da sonst zu viele Anfragen notwendig wären.

Eine weitere Art des Datenzugriffs ist mit den JSON- und RDF-Gesamtexporten möglich, die wöchentlich erstellt und von Wikidata zum Download angeboten werden.
Für die Verarbeitung der JSON-Exporte und die Generierung von RDF existiert bereits eine Java-Bibliothek, Wikidata-Toolkit.\footnote{\url{https://github.com/Wikidata/Wikidata-Toolkit}}
Auch Optionen für die Generierung der RDF-Daten, wie eine Einschränkung auf truthy-Tripel, sind in Wikidata-Toolkit schon implementiert.
Wikidata-Toolkit erlaubt so eine einfache Filterung der Dumps.
Die Optionen decken jedoch nicht alle Anforderungen ab (die Filterung der zu exportierenden Statements ist zum Beispiel nicht vorgesehen), sodass Wikidata-Toolkit lediglich als Grundlage für die Implementierung des Systems dienen kann.
Auch bietet Wikidata-Toolkit weder ein User-Interface noch Möglichkeiten zur Archivierung.

Ein gezielter Zugriff auf die Daten ist mit dem Wikidata Query Service \cite{wd-sparql} möglich.
Mit der verwendeten Abfragesprache SPARQL \cite{sparql} lassen sich die in den Anforderungen beschriebenen Filter realisieren.
Die Abfrage der gesamten Daten von Items ist mit dem Query Service allerdings schwierig.
Da der Query Service auf der RDF-Darstellung von Wikidata basiert, müssen für jedes Item noch explizit die Statements, Values und Referenzen abgefragt werden.
Die Konstruktion dieser Abfrage von Hand ist wenig nutzerfreundlich.
Des Weiteren ist die Performance dieser Abfrage ein Problem.
Bei der Extraktion aller Daten von Items, die Instanz der Klasse Mensch (\verb|Q5|) sind, wird das Timeout der Query Service von einer Minute schon bei weniger als 3000 Items erreicht.\footnote{Die für den Test verwendete Abfrage zeigt \cref{lst:sparql-persons} im Appendix. Am 26.10.2019 hat das Limit 3000 ein Timeout verursacht, während die Abfrage mit Limit 2500 innerhalb des Timeouts terminierte.}
Die Archivierung wird bei diesem Ansatz ebenso nicht abgedeckt.
Die Daten des Query Services werden live aktualisiert, die Ergebnisse einer Abfrage sind daher zu einem späteren Zeitpunkt verschieden.

Eine zweite Schnittstelle, die auf der RDF-Darstellung basiert, sind Triple Pattern Fragments, welche eine Form von Linked Data Fragments \cite{ldf} darstellen.
Mit der Triple Pattern Fragments Schnittstelle von Wikidata können einfache Abfragen gestellt werden die nach Tripeln mit einer bestimmten Kombination aus Subjekt, Prädikat und Objekt suchen.
Nicht alle Felder müssen angegeben werden, eine Abfragen kann zum Beispiel nach allen Tripeln mit Prädikat \verb|rdfs:label| suchen.
Da die Abfragen sehr einfach sind, erfordert die Auswertung dieser Abfragen wenig Ressourcen auf dem Server und es ist keine Laufzeitlimit notwendig.
Problematisch ist bei dieser Schnittstelle die Extraktion von Statements, Values und Referenzen.
Im Gegensatz zu SPARQL sind Triple Pattern Fragments nicht mächtig genug, um in einer Abfrage alle Tripel eines Items inklusive der Tripel für Statements dieses Items abzufragen.
Als Möglichkeiten bleiben nur die Abfrage der Tripel für alle Statements oder die einzelne Abfrage der Tripel jedes Statements.
Beide Varianten sind nicht praktikabel, da entweder zu viele Daten abgefragt werden oder zu viele Abfragen notwendig sind.

Für die Auswahl von Entities für den Export ist PetScan\footnote{\url{https://petscan.wmflabs.org/}} interessant.
Dieses Tool bietet ein User-Interface mit einer Vielzahl an Optionen, um Listen von Wikipedia bzw. Wikidata-Seiten zu erstellen. 
Das Tool bietet keinen Optionen zum Export der Entities als RDF an, es lassen sich damit keine Dumps erstellen.
Es liefert jedoch eine Vielzahl an Ideen für mögliche Filterkriterien.

Eine andere Variante, die Größe der Dumps zu reduzieren und damit den Zugang zu erleichern, zeigt DBpedia \cite{dbpedia}.
DBpedia partioniert seine Dumps dazu in mehrere, kleinere Dumps.
Auch einige aus Wikidata abgeleitete Dumps sind verfügbar\footnote{\url{https://databus.dbpedia.org/dbpedia/wikidata/}}.
Die Daten in DBpedia stammen aus definierten Extraktionen von Wikimedia Projekten, die jeweils nur bestimmte Relationen extrahieren.
Durch das klar definierte Schema ist damit eine Trennung der Datensätze nach der Art der Extraktion möglich.
Für Wikidata ist dieser Ansatz deutlich schwieriger, da das Datenschema so flexibel ist und die Relationen (Properties) vorher nicht feststehen.
Außerdem gibt es neben den Relationen noch weitere Aspekte, nach denen eine Trennung erfolgen könnte, zum Beispiel ob Statements Referenzen haben oder nicht.
Die Definition von Kriterien zur Partionierung von Wikidata ist daher schwierig.

HDT \cite{hdt}, ein Binärformat für RDF-Daten, verwendet eine optimierte, komprimierte Kodierung zur Speicherung der Dumps.
Auf diesem Format lassen sich einfache SPARQL-Abfragen effizient ausführen \cite{hdt-query}.
HDT-Dumps sind damit kleiner und erlauben trotzdem noch die Filterung nach einfachen Kriterien.
Leider erfordert die Erstellung von HDT-Dumps viel Arbeitsspeicher und Rechenzeit, besonders bei einer so großen Datenmenge wie Wikidata.
Durch die komprimierte Kodierung ist eine inkrementelle Aktualisierung von HDT-Dumps nicht möglich, sodass der komplette Dump regelmäßig neu erstellt werden müsste.

